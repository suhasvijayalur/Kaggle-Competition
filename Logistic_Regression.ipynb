{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Times New Roman;\">Kaggle Competition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Times New Roman;\"><b><font size = \"4\">Suhas Alur</font></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family:Times New Roman;\">Loading & Pre-processing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suhas\\Documents\\MSBA\\Semester Two\\Predictive Analytics\\Homework\\Homework - 5\\Data\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries and specify that graphs should be plotted inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "%cd 'C:\\Users\\Suhas\\Documents\\MSBA\\Semester Two\\Predictive Analytics\\Homework\\Homework - 5\\Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"font-family:Times New Roman;\">Loading the training dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the train dataset\n",
    "data_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset based on attributes and target variable\n",
    "X = data_train.iloc[:,0:25]\n",
    "y = data_train.adopter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86682\n",
      "1540\n"
     ]
    }
   ],
   "source": [
    "print len(y)\n",
    "print sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the train data into training and testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60677\n",
      "1101\n"
     ]
    }
   ],
   "source": [
    "print len(y_train)\n",
    "print sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26005\n",
      "439\n"
     ]
    }
   ],
   "source": [
    "print len(y_test)\n",
    "print sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using SMOTE to oversample the minority class\n",
    "#from imblearn.combine import SMOTEENN\n",
    "\n",
    "#smote = SMOTEENN(random_state = 42)\n",
    "\n",
    "#smote = SMOTE(kind='regular', n_jobs = -1, m_neighbors = 20, k_neighbors = 10)\n",
    "#smox, smoy = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#smox, smoy = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print len(smoy)\n",
    "#print sum(smoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"font-family:Times New Roman;\">Fitting the model using the train dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state = 9, C = 0.001 , n_jobs = -1, penalty = 'l1', class_weight = 'balanced', intercept_scaling  = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalizing the data before fitting the data\n",
    "#from sklearn import preprocessing\n",
    "\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#smox = min_max_scaler.fit_transform(smox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"font-family:Times New Roman;\">Cross-validating the model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Using cross validation to understand if the model has overfit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X_train, y_train, cv=3)\n",
    "\n",
    "# Accuracy of the cross-validation\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fitting the model on 70% dataset\n",
    "model = model.fit(X_train, y_train)\n",
    "#predicted = model.predict(smox)\n",
    "# AUC\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "#print roc_auc_score(smoy, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26005\n",
      "439\n"
     ]
    }
   ],
   "source": [
    "print len(y_test)\n",
    "print sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the test data to check prediction accuracy\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'prediction_gbc':predicted})\n",
    "submission.to_csv(\"prediction_logistic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.79      0.88     25566\n",
      "    class 1       0.04      0.53      0.08       439\n",
      "\n",
      "avg / total       0.97      0.78      0.86     26005\n",
      "\n",
      "[[20078  5488]\n",
      " [  207   232]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, predicted, target_names=target_names))\n",
    "print confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuEAAAIACAYAAAAsWLK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngM3+DyWObHwti1ShAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10988780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ggplot: (15406403)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from ggplot import *\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, predicted)\n",
    "\n",
    "df = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n",
    "\n",
    "ggplot(df, aes(x='fpr', y='tpr')) +\\\n",
    "    geom_line() +\\\n",
    "    geom_abline(linetype='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the test data to check prediction accuracy\n",
    "predicted = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff = 0.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.00      0.02      0.00     26005\n",
      "\n",
      "[[    0 25566]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.01\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    4 25562]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.02\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    4 25562]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.03\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    4 25562]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.04\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    5 25561]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.05\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    6 25560]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.06\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    7 25559]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.07\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    7 25559]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.08\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    8 25558]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.09\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    8 25558]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[    9 25557]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.11\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   10 25556]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.12\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   11 25555]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.13\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   11 25555]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.14\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   11 25555]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.15\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   13 25553]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.16\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   13 25553]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.17\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   13 25553]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.18\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   14 25552]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.19\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   16 25550]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.02      0.00     26005\n",
      "\n",
      "[[   17 25549]\n",
      " [    0   439]]\n",
      "*********************************************************\n",
      "Cutoff = 0.21\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.95      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.93      0.02      0.00     26005\n",
      "\n",
      "[[   18 25548]\n",
      " [    1   438]]\n",
      "*********************************************************\n",
      "Cutoff = 0.22\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.95      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.94      0.02      0.00     26005\n",
      "\n",
      "[[   20 25546]\n",
      " [    1   438]]\n",
      "*********************************************************\n",
      "Cutoff = 0.23\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.96      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.95      0.02      0.00     26005\n",
      "\n",
      "[[   25 25541]\n",
      " [    1   438]]\n",
      "*********************************************************\n",
      "Cutoff = 0.24\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.97      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.95      0.02      0.00     26005\n",
      "\n",
      "[[   32 25534]\n",
      " [    1   438]]\n",
      "*********************************************************\n",
      "Cutoff = 0.25\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.00      0.00     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.96      0.02      0.00     26005\n",
      "\n",
      "[[   52 25514]\n",
      " [    1   438]]\n",
      "*********************************************************\n",
      "Cutoff = 0.26\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.00      0.01     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.97      0.02      0.01     26005\n",
      "\n",
      "[[  112 25454]\n",
      " [    1   438]]\n",
      "*********************************************************\n",
      "Cutoff = 0.27\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.01      0.02     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.03      0.02     26005\n",
      "\n",
      "[[  286 25280]\n",
      " [    2   437]]\n",
      "*********************************************************\n",
      "Cutoff = 0.28\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.02      0.04     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.04      0.04     26005\n",
      "\n",
      "[[  588 24978]\n",
      " [    2   437]]\n",
      "*********************************************************\n",
      "Cutoff = 0.29\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.04      0.08     25566\n",
      "    class 1       0.02      1.00      0.03       439\n",
      "\n",
      "avg / total       0.98      0.06      0.08     26005\n",
      "\n",
      "[[ 1103 24463]\n",
      " [    2   437]]\n",
      "*********************************************************\n",
      "Cutoff = 0.3\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.07      0.14     25566\n",
      "    class 1       0.02      1.00      0.04       439\n",
      "\n",
      "avg / total       0.98      0.09      0.14     26005\n",
      "\n",
      "[[ 1882 23684]\n",
      " [    2   437]]\n",
      "*********************************************************\n",
      "Cutoff = 0.31\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.11      0.20     25566\n",
      "    class 1       0.02      0.99      0.04       439\n",
      "\n",
      "avg / total       0.98      0.13      0.20     26005\n",
      "\n",
      "[[ 2916 22650]\n",
      " [    4   435]]\n",
      "*********************************************************\n",
      "Cutoff = 0.32\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.16      0.28     25566\n",
      "    class 1       0.02      0.98      0.04       439\n",
      "\n",
      "avg / total       0.98      0.18      0.27     26005\n",
      "\n",
      "[[ 4129 21437]\n",
      " [    9   430]]\n",
      "*********************************************************\n",
      "Cutoff = 0.33\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.22      0.36     25566\n",
      "    class 1       0.02      0.97      0.04       439\n",
      "\n",
      "avg / total       0.98      0.23      0.35     26005\n",
      "\n",
      "[[ 5530 20036]\n",
      " [   13   426]]\n",
      "*********************************************************\n",
      "Cutoff = 0.34\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.27      0.43     25566\n",
      "    class 1       0.02      0.96      0.04       439\n",
      "\n",
      "avg / total       0.98      0.28      0.42     26005\n",
      "\n",
      "[[ 6921 18645]\n",
      " [   19   420]]\n",
      "*********************************************************\n",
      "Cutoff = 0.35\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.32      0.49     25566\n",
      "    class 1       0.02      0.94      0.05       439\n",
      "\n",
      "avg / total       0.98      0.33      0.48     26005\n",
      "\n",
      "[[ 8276 17290]\n",
      " [   27   412]]\n",
      "*********************************************************\n",
      "Cutoff = 0.36\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.37      0.54     25566\n",
      "    class 1       0.02      0.92      0.05       439\n",
      "\n",
      "avg / total       0.98      0.38      0.54     26005\n",
      "\n",
      "[[ 9579 15987]\n",
      " [   37   402]]\n",
      "*********************************************************\n",
      "Cutoff = 0.37\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       1.00      0.42      0.59     25566\n",
      "    class 1       0.03      0.88      0.05       439\n",
      "\n",
      "avg / total       0.98      0.43      0.58     26005\n",
      "\n",
      "[[10809 14757]\n",
      " [   53   386]]\n",
      "*********************************************************\n",
      "Cutoff = 0.38\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.47      0.63     25566\n",
      "    class 1       0.03      0.85      0.05       439\n",
      "\n",
      "avg / total       0.98      0.47      0.62     26005\n",
      "\n",
      "[[11916 13650]\n",
      " [   65   374]]\n",
      "*********************************************************\n",
      "Cutoff = 0.39\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.51      0.67     25566\n",
      "    class 1       0.03      0.82      0.05       439\n",
      "\n",
      "avg / total       0.98      0.51      0.66     26005\n",
      "\n",
      "[[12999 12567]\n",
      " [   80   359]]\n",
      "*********************************************************\n",
      "Cutoff = 0.4\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.55      0.71     25566\n",
      "    class 1       0.03      0.79      0.06       439\n",
      "\n",
      "avg / total       0.98      0.55      0.70     26005\n",
      "\n",
      "[[14011 11555]\n",
      " [   92   347]]\n",
      "*********************************************************\n",
      "Cutoff = 0.41\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.58      0.74     25566\n",
      "    class 1       0.03      0.77      0.06       439\n",
      "\n",
      "avg / total       0.98      0.59      0.72     26005\n",
      "\n",
      "[[14924 10642]\n",
      " [  103   336]]\n",
      "*********************************************************\n",
      "Cutoff = 0.42\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.61      0.76     25566\n",
      "    class 1       0.03      0.74      0.06       439\n",
      "\n",
      "avg / total       0.98      0.62      0.75     26005\n",
      "\n",
      "[[15715  9851]\n",
      " [  114   325]]\n",
      "*********************************************************\n",
      "Cutoff = 0.43\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.64      0.78     25566\n",
      "    class 1       0.03      0.72      0.06       439\n",
      "\n",
      "avg / total       0.98      0.65      0.77     26005\n",
      "\n",
      "[[16476  9090]\n",
      " [  125   314]]\n",
      "*********************************************************\n",
      "Cutoff = 0.44\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.67      0.80     25566\n",
      "    class 1       0.03      0.69      0.07       439\n",
      "\n",
      "avg / total       0.98      0.67      0.79     26005\n",
      "\n",
      "[[17139  8427]\n",
      " [  138   301]]\n",
      "*********************************************************\n",
      "Cutoff = 0.45\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.69      0.82     25566\n",
      "    class 1       0.04      0.66      0.07       439\n",
      "\n",
      "avg / total       0.98      0.69      0.80     26005\n",
      "\n",
      "[[17736  7830]\n",
      " [  150   289]]\n",
      "*********************************************************\n",
      "Cutoff = 0.46\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.71      0.83     25566\n",
      "    class 1       0.04      0.63      0.07       439\n",
      "\n",
      "avg / total       0.98      0.71      0.82     26005\n",
      "\n",
      "[[18271  7295]\n",
      " [  161   278]]\n",
      "*********************************************************\n",
      "Cutoff = 0.47\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.73      0.84     25566\n",
      "    class 1       0.04      0.61      0.07       439\n",
      "\n",
      "avg / total       0.97      0.73      0.83     26005\n",
      "\n",
      "[[18777  6789]\n",
      " [  173   266]]\n",
      "*********************************************************\n",
      "Cutoff = 0.48\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.75      0.85     25566\n",
      "    class 1       0.04      0.58      0.07       439\n",
      "\n",
      "avg / total       0.97      0.75      0.84     26005\n",
      "\n",
      "[[19222  6344]\n",
      " [  184   255]]\n",
      "*********************************************************\n",
      "Cutoff = 0.49\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.77      0.87     25566\n",
      "    class 1       0.04      0.55      0.07       439\n",
      "\n",
      "avg / total       0.97      0.76      0.85     26005\n",
      "\n",
      "[[19650  5916]\n",
      " [  199   240]]\n",
      "*********************************************************\n",
      "Cutoff = 0.5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.79      0.88     25566\n",
      "    class 1       0.04      0.53      0.08       439\n",
      "\n",
      "avg / total       0.97      0.78      0.86     26005\n",
      "\n",
      "[[20078  5488]\n",
      " [  207   232]]\n",
      "*********************************************************\n",
      "Cutoff = 0.51\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.80      0.89     25566\n",
      "    class 1       0.04      0.50      0.08       439\n",
      "\n",
      "avg / total       0.97      0.80      0.87     26005\n",
      "\n",
      "[[20474  5092]\n",
      " [  218   221]]\n",
      "*********************************************************\n",
      "Cutoff = 0.52\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.81      0.89     25566\n",
      "    class 1       0.04      0.48      0.08       439\n",
      "\n",
      "avg / total       0.97      0.81      0.88     26005\n",
      "\n",
      "[[20800  4766]\n",
      " [  228   211]]\n",
      "*********************************************************\n",
      "Cutoff = 0.53\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.83      0.90     25566\n",
      "    class 1       0.04      0.46      0.08       439\n",
      "\n",
      "avg / total       0.97      0.82      0.89     26005\n",
      "\n",
      "[[21129  4437]\n",
      " [  237   202]]\n",
      "*********************************************************\n",
      "Cutoff = 0.54\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.84      0.91     25566\n",
      "    class 1       0.04      0.44      0.08       439\n",
      "\n",
      "avg / total       0.97      0.83      0.89     26005\n",
      "\n",
      "[[21415  4151]\n",
      " [  245   194]]\n",
      "*********************************************************\n",
      "Cutoff = 0.55\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.85      0.91     25566\n",
      "    class 1       0.05      0.42      0.08       439\n",
      "\n",
      "avg / total       0.97      0.84      0.90     26005\n",
      "\n",
      "[[21655  3911]\n",
      " [  253   186]]\n",
      "*********************************************************\n",
      "Cutoff = 0.56\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.86      0.92     25566\n",
      "    class 1       0.05      0.41      0.09       439\n",
      "\n",
      "avg / total       0.97      0.85      0.90     26005\n",
      "\n",
      "[[21922  3644]\n",
      " [  257   182]]\n",
      "*********************************************************\n",
      "Cutoff = 0.57\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.87      0.92     25566\n",
      "    class 1       0.05      0.40      0.09       439\n",
      "\n",
      "avg / total       0.97      0.86      0.91     26005\n",
      "\n",
      "[[22148  3418]\n",
      " [  263   176]]\n",
      "*********************************************************\n",
      "Cutoff = 0.58\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.88      0.93     25566\n",
      "    class 1       0.05      0.38      0.09       439\n",
      "\n",
      "avg / total       0.97      0.87      0.91     26005\n",
      "\n",
      "[[22374  3192]\n",
      " [  270   169]]\n",
      "*********************************************************\n",
      "Cutoff = 0.59\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.88      0.93     25566\n",
      "    class 1       0.05      0.38      0.09       439\n",
      "\n",
      "avg / total       0.97      0.87      0.92     26005\n",
      "\n",
      "[[22560  3006]\n",
      " [  274   165]]\n",
      "*********************************************************\n",
      "Cutoff = 0.6\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.89      0.94     25566\n",
      "    class 1       0.05      0.37      0.09       439\n",
      "\n",
      "avg / total       0.97      0.88      0.92     26005\n",
      "\n",
      "[[22738  2828]\n",
      " [  278   161]]\n",
      "*********************************************************\n",
      "Cutoff = 0.61\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.90      0.94     25566\n",
      "    class 1       0.05      0.34      0.09       439\n",
      "\n",
      "avg / total       0.97      0.89      0.93     26005\n",
      "\n",
      "[[22904  2662]\n",
      " [  289   150]]\n",
      "*********************************************************\n",
      "Cutoff = 0.62\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.90      0.94     25566\n",
      "    class 1       0.05      0.32      0.09       439\n",
      "\n",
      "avg / total       0.97      0.89      0.93     26005\n",
      "\n",
      "[[23089  2477]\n",
      " [  297   142]]\n",
      "*********************************************************\n",
      "Cutoff = 0.63\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.91      0.95     25566\n",
      "    class 1       0.06      0.32      0.10       439\n",
      "\n",
      "avg / total       0.97      0.90      0.93     26005\n",
      "\n",
      "[[23248  2318]\n",
      " [  300   139]]\n",
      "*********************************************************\n",
      "Cutoff = 0.64\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.92      0.95     25566\n",
      "    class 1       0.06      0.30      0.10       439\n",
      "\n",
      "avg / total       0.97      0.91      0.94     26005\n",
      "\n",
      "[[23405  2161]\n",
      " [  307   132]]\n",
      "*********************************************************\n",
      "Cutoff = 0.65\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.92      0.95     25566\n",
      "    class 1       0.06      0.29      0.10       439\n",
      "\n",
      "avg / total       0.97      0.91      0.94     26005\n",
      "\n",
      "[[23531  2035]\n",
      " [  313   126]]\n",
      "*********************************************************\n",
      "Cutoff = 0.66\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.92      0.95     25566\n",
      "    class 1       0.06      0.27      0.09       439\n",
      "\n",
      "avg / total       0.97      0.91      0.94     26005\n",
      "\n",
      "[[23637  1929]\n",
      " [  321   118]]\n",
      "*********************************************************\n",
      "Cutoff = 0.67\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.93      0.96     25566\n",
      "    class 1       0.06      0.25      0.09       439\n",
      "\n",
      "avg / total       0.97      0.92      0.94     26005\n",
      "\n",
      "[[23761  1805]\n",
      " [  328   111]]\n",
      "*********************************************************\n",
      "Cutoff = 0.68\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.93      0.96     25566\n",
      "    class 1       0.06      0.23      0.09       439\n",
      "\n",
      "avg / total       0.97      0.92      0.94     26005\n",
      "\n",
      "[[23891  1675]\n",
      " [  337   102]]\n",
      "*********************************************************\n",
      "Cutoff = 0.69\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.94      0.96     25566\n",
      "    class 1       0.06      0.23      0.10       439\n",
      "\n",
      "avg / total       0.97      0.93      0.95     26005\n",
      "\n",
      "[[24000  1566]\n",
      " [  338   101]]\n",
      "*********************************************************\n",
      "Cutoff = 0.7\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.94      0.96     25566\n",
      "    class 1       0.06      0.21      0.09       439\n",
      "\n",
      "avg / total       0.97      0.93      0.95     26005\n",
      "\n",
      "[[24087  1479]\n",
      " [  347    92]]\n",
      "*********************************************************\n",
      "Cutoff = 0.71\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.95      0.97     25566\n",
      "    class 1       0.06      0.20      0.09       439\n",
      "\n",
      "avg / total       0.97      0.93      0.95     26005\n",
      "\n",
      "[[24178  1388]\n",
      " [  350    89]]\n",
      "*********************************************************\n",
      "Cutoff = 0.72\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.95      0.97     25566\n",
      "    class 1       0.06      0.19      0.09       439\n",
      "\n",
      "avg / total       0.97      0.94      0.95     26005\n",
      "\n",
      "[[24256  1310]\n",
      " [  354    85]]\n",
      "*********************************************************\n",
      "Cutoff = 0.73\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.95      0.97     25566\n",
      "    class 1       0.06      0.19      0.09       439\n",
      "\n",
      "avg / total       0.97      0.94      0.95     26005\n",
      "\n",
      "[[24322  1244]\n",
      " [  356    83]]\n",
      "*********************************************************\n",
      "Cutoff = 0.74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.95      0.97     25566\n",
      "    class 1       0.06      0.17      0.09       439\n",
      "\n",
      "avg / total       0.97      0.94      0.95     26005\n",
      "\n",
      "[[24392  1174]\n",
      " [  363    76]]\n",
      "*********************************************************\n",
      "Cutoff = 0.75\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.96      0.97     25566\n",
      "    class 1       0.06      0.16      0.09       439\n",
      "\n",
      "avg / total       0.97      0.94      0.96     26005\n",
      "\n",
      "[[24450  1116]\n",
      " [  367    72]]\n",
      "*********************************************************\n",
      "Cutoff = 0.76\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.96      0.97     25566\n",
      "    class 1       0.06      0.16      0.09       439\n",
      "\n",
      "avg / total       0.97      0.95      0.96     26005\n",
      "\n",
      "[[24525  1041]\n",
      " [  368    71]]\n",
      "*********************************************************\n",
      "Cutoff = 0.77\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.96      0.97     25566\n",
      "    class 1       0.07      0.16      0.09       439\n",
      "\n",
      "avg / total       0.97      0.95      0.96     26005\n",
      "\n",
      "[[24594   972]\n",
      " [  369    70]]\n",
      "*********************************************************\n",
      "Cutoff = 0.78\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.99      0.96      0.97     25566\n",
      "    class 1       0.06      0.15      0.09       439\n",
      "\n",
      "avg / total       0.97      0.95      0.96     26005\n",
      "\n",
      "[[24645   921]\n",
      " [  375    64]]\n",
      "*********************************************************\n",
      "Cutoff = 0.79\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.97      0.98     25566\n",
      "    class 1       0.06      0.13      0.09       439\n",
      "\n",
      "avg / total       0.97      0.95      0.96     26005\n",
      "\n",
      "[[24691   875]\n",
      " [  380    59]]\n",
      "*********************************************************\n",
      "Cutoff = 0.8\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.97      0.98     25566\n",
      "    class 1       0.06      0.13      0.08       439\n",
      "\n",
      "avg / total       0.97      0.95      0.96     26005\n",
      "\n",
      "[[24747   819]\n",
      " [  384    55]]\n",
      "*********************************************************\n",
      "Cutoff = 0.81\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.97      0.98     25566\n",
      "    class 1       0.07      0.12      0.09       439\n",
      "\n",
      "avg / total       0.97      0.96      0.96     26005\n",
      "\n",
      "[[24797   769]\n",
      " [  385    54]]\n",
      "*********************************************************\n",
      "Cutoff = 0.82\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.97      0.98     25566\n",
      "    class 1       0.07      0.12      0.09       439\n",
      "\n",
      "avg / total       0.97      0.96      0.96     26005\n",
      "\n",
      "[[24853   713]\n",
      " [  387    52]]\n",
      "*********************************************************\n",
      "Cutoff = 0.83\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.97      0.98     25566\n",
      "    class 1       0.07      0.11      0.08       439\n",
      "\n",
      "avg / total       0.97      0.96      0.96     26005\n",
      "\n",
      "[[24889   677]\n",
      " [  390    49]]\n",
      "*********************************************************\n",
      "Cutoff = 0.84\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.98      0.98     25566\n",
      "    class 1       0.07      0.10      0.08       439\n",
      "\n",
      "avg / total       0.97      0.96      0.96     26005\n",
      "\n",
      "[[24941   625]\n",
      " [  393    46]]\n",
      "*********************************************************\n",
      "Cutoff = 0.85\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.98      0.98     25566\n",
      "    class 1       0.07      0.10      0.08       439\n",
      "\n",
      "avg / total       0.97      0.96      0.97     26005\n",
      "\n",
      "[[24984   582]\n",
      " [  396    43]]\n",
      "*********************************************************\n",
      "Cutoff = 0.86\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.98      0.98     25566\n",
      "    class 1       0.07      0.09      0.08       439\n",
      "\n",
      "avg / total       0.97      0.96      0.97     26005\n",
      "\n",
      "[[25021   545]\n",
      " [  398    41]]\n",
      "*********************************************************\n",
      "Cutoff = 0.87\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.98      0.98     25566\n",
      "    class 1       0.07      0.08      0.08       439\n",
      "\n",
      "avg / total       0.97      0.97      0.97     26005\n",
      "\n",
      "[[25061   505]\n",
      " [  402    37]]\n",
      "*********************************************************\n",
      "Cutoff = 0.88\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.98      0.98     25566\n",
      "    class 1       0.07      0.08      0.08       439\n",
      "\n",
      "avg / total       0.97      0.97      0.97     26005\n",
      "\n",
      "[[25095   471]\n",
      " [  403    36]]\n",
      "*********************************************************\n",
      "Cutoff = 0.89\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.98      0.98     25566\n",
      "    class 1       0.07      0.08      0.08       439\n",
      "\n",
      "avg / total       0.97      0.97      0.97     26005\n",
      "\n",
      "[[25131   435]\n",
      " [  404    35]]\n",
      "*********************************************************\n",
      "Cutoff = 0.9\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.98      0.98     25566\n",
      "    class 1       0.08      0.08      0.08       439\n",
      "\n",
      "avg / total       0.97      0.97      0.97     26005\n",
      "\n",
      "[[25169   397]\n",
      " [  405    34]]\n",
      "*********************************************************\n",
      "Cutoff = 0.91\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.99      0.98     25566\n",
      "    class 1       0.08      0.07      0.07       439\n",
      "\n",
      "avg / total       0.97      0.97      0.97     26005\n",
      "\n",
      "[[25199   367]\n",
      " [  408    31]]\n",
      "*********************************************************\n",
      "Cutoff = 0.92\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.99      0.99     25566\n",
      "    class 1       0.08      0.07      0.07       439\n",
      "\n",
      "avg / total       0.97      0.97      0.97     26005\n",
      "\n",
      "[[25236   330]\n",
      " [  410    29]]\n",
      "*********************************************************\n",
      "Cutoff = 0.93\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.99      0.99     25566\n",
      "    class 1       0.09      0.06      0.07       439\n",
      "\n",
      "avg / total       0.97      0.97      0.97     26005\n",
      "\n",
      "[[25290   276]\n",
      " [  412    27]]\n",
      "*********************************************************\n",
      "Cutoff = 0.94\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.99      0.99     25566\n",
      "    class 1       0.09      0.06      0.07       439\n",
      "\n",
      "avg / total       0.97      0.97      0.97     26005\n",
      "\n",
      "[[25323   243]\n",
      " [  414    25]]\n",
      "*********************************************************\n",
      "Cutoff = 0.95\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.99      0.99     25566\n",
      "    class 1       0.09      0.05      0.06       439\n",
      "\n",
      "avg / total       0.97      0.98      0.97     26005\n",
      "\n",
      "[[25350   216]\n",
      " [  417    22]]\n",
      "*********************************************************\n",
      "Cutoff = 0.96\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.99      0.99     25566\n",
      "    class 1       0.10      0.05      0.07       439\n",
      "\n",
      "avg / total       0.97      0.98      0.97     26005\n",
      "\n",
      "[[25386   180]\n",
      " [  418    21]]\n",
      "*********************************************************\n",
      "Cutoff = 0.97\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      0.99      0.99     25566\n",
      "    class 1       0.11      0.04      0.06       439\n",
      "\n",
      "avg / total       0.97      0.98      0.97     26005\n",
      "\n",
      "[[25408   158]\n",
      " [  420    19]]\n",
      "*********************************************************\n",
      "Cutoff = 0.98\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      1.00      0.99     25566\n",
      "    class 1       0.11      0.04      0.05       439\n",
      "\n",
      "avg / total       0.97      0.98      0.97     26005\n",
      "\n",
      "[[25439   127]\n",
      " [  423    16]]\n",
      "*********************************************************\n",
      "Cutoff = 0.99\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.98      1.00      0.99     25566\n",
      "    class 1       0.13      0.03      0.05       439\n",
      "\n",
      "avg / total       0.97      0.98      0.97     26005\n",
      "\n",
      "[[25479    87]\n",
      " [  426    13]]\n",
      "*********************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def drange(start, stop, step):\n",
    "    while start < stop:\n",
    "            yield start\n",
    "            start += step\n",
    "\n",
    "\n",
    "for j in drange(0.00, 1.00, 0.01):\n",
    "    cutoff = j\n",
    "    y_pred = list()\n",
    "    for i in range(0,len(predicted[:,1])):\n",
    "        if predicted[:,1][i] > cutoff:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    target_names = ['class 0', 'class 1']\n",
    "    print \"Cutoff = {}\".format(cutoff)\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    print confusion_matrix(y_test, y_pred)\n",
    "    print(\"*********************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"font-family:Times New Roman;\">Loading the test dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the train dataset\n",
    "data_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taking only the required attributes\n",
    "X_test = data_test.iloc[:,0:25]\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the data before fitting the data\n",
    "#from sklearn import preprocessing\n",
    "\n",
    "#min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#X_test = min_max_scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"font-family:Times New Roman;\">Predicting</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predicted = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutoff = 0.69\n",
    "y_pred = list()\n",
    "for i in range(0,len(predicted[:,1])):\n",
    "    if predicted[:,1][i] > cutoff:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86681\n",
      "5748\n"
     ]
    }
   ],
   "source": [
    "print len(y_pred)\n",
    "print sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'user_id': data_test['user_id'], 'prediction(adopter)':y_pred})\n",
    "submission = submission[['user_id', 'prediction(adopter)']]\n",
    "submission.to_csv(\"logistic_regression_new_20Nov.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
